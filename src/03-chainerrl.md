# まえがき

うめざわ「ここでは、ChainerRLの説明をするよ」

ちぇいなーちゃん「CVは分かったけど、RLって何なのよ。東京駅で売ってるやつ？」

うめざわ「それは、ワッフルケーキだよね。RLは、Reinforcement Learningの略で、強化学習だよ。」

※ 本書は、東京駅で売っているカラフルなワッフルケーキ詰め合わせとは本当に何の関係もありません。

ちぇいなーちゃん「強化学習って？」

うめざわ「一般的に、機械学習って、教師あり学習・教師なし学習に分かれるよね。」

ちぇいなーちゃん「ふむふむ」

うめざわ「そうじゃないのが強化学習。それぞれこんな感じの特徴がある。」

* 教師あり学習
  * 入出力の関係を学習する（正解データが与えられる）
  * 一般的にイメージする機械学習（特にDeep learning）はこれが多い
* 教師なし学習
  * 正解データが与えられない
  * クラスタリング、異常検知など
* 強化学習
  * 試行錯誤を通じて価値を最大化する
  * 入力は乱数、正解データは与えられない（環境と報酬が与えられる）

ちぇいなーちゃん「そうじゃないのがって、ずいぶんとおざなりね。どういう時に使うの？」

うめざわ「そうだね、具体例から入った方が分かりやすいかな。強化学習で近年話題になったものに、Atariのゲームを人工知能で解いたっというのがあるんだ。DQNっていうんだけど、これにはゲームのルールを全く与えずに、繰り返しゲームをプレイするだけで、ブロック崩しとかがプレイできるようになったんだ」

![強化学習でブロック崩しを学習できる](src/images/chainerrl_fig1.png)

https://youtu.be/TmPfTpjtdgg

ちぇいなーちゃん「DQNって、、、」

うめざわ「そのDQNじゃないよ！Deep Q Networkの略だね。ゲームのルールを教えないけど、それがプレイできるようになるのってすごくない？」

ちぇいなーちゃん「すごいけど、画像認識だって、認識のルール教えてないわよ。同じじゃない？」

うめざわ「そうだね。でも画像認識は、正解があるよね。強化学習は、例えば、ゲームの画面と得点だけで、最適な行動を学習できる。もしこれを教師あり学習でやろうと思ったら、正解行動のデータが必要だけど、ブロック崩しみたいなゲームで正解行動のデータを作るとか難しいよね」

ちぇいなーちゃん「あー、なるほど。確かに、それができたらすごい気がしてきた。」

うめざわ「長々と話しちゃったけど、そんな感じのが強化学習。で、その強化学習を簡単にできるのが、ChainerRLなんだ」

ちぇいなーちゃん「やっと登場ね。」

そんなChainerRLですが、うめざわさんの言うとおり、強化学習を簡単に使えます。

ChainerRLの説明の前に、もう一つ、説明しなければならないものがあります。
うめざわさんも少し触れていましたが、強化学習には、エージェントと環境があります。

![強化学習の要素](src/images/chainerrl_fig2.png)

エージェントは、行動を決めるもので、ChainerRLではその内部で用意されています。
一方、環境は、シミュレーション環境等です。例えば、ブロック崩しの場合は、そのエミュレータが環境になります。
この環境とエージェントの間のI/Fが統一されていれば、エージェントのプログラムを変えずに、環境を入れ換えることができるはずです。

それを実現しているのがOpen AI Gymです。

手持ちの環境に、Open AI Gymを入れて見ましょう。

```
$ pip install gym
```

そうすると、

```
import gym
env = gym.make("Breakout-v0") # 1. 環境を作成
observation = env.reset() # 2. リセット。状態 s を受け取る
for _ in range(1000):　
  env.render() # 3. 描画する
  action = env.action_space.sample() # 4. アクションを決定。ここではランダムサンプリング
  observation, reward, done, info = env.step(action) # ステップを実行し、状態、報酬、終了フラグを得る
```
